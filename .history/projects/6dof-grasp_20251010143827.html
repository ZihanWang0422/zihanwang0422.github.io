<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>6-DOF Robotic Grasping - Zihan Wang</title>
  <meta name="description" content="6-DOF Robotic Grasping Project by Zihan Wang">
  
  <link rel="shortcut icon" href="../images/android-chrome-512x512.png">
  <link rel="stylesheet" href="../main.css">
  <link rel="stylesheet" href="../css/style.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  
  <script src="../js/jquery-2.1.3.min.js"></script>
  <script src="../js/theme.js"></script>

  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      line-height: 1.6;
      color: #333;
      background-color: #f9f9f9;
    }

    .project-header {
      background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
      color: white;
      padding: 60px 20px;
      text-align: center;
      margin-bottom: 40px;
    }

    .project-title {
      font-size: 2.5em;
      font-weight: 700;
      margin-bottom: 20px;
      text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
    }

    .project-authors {
      font-size: 1.2em;
      margin-bottom: 10px;
      opacity: 0.95;
    }

    .project-affiliation {
      font-size: 1em;
      opacity: 0.9;
      font-style: italic;
    }

    .project-links {
      margin-top: 25px;
      display: flex;
      justify-content: center;
      gap: 15px;
      flex-wrap: wrap;
    }

    .project-link {
      display: inline-block;
      padding: 10px 25px;
      background-color: rgba(255, 255, 255, 0.2);
      color: white;
      text-decoration: none;
      border-radius: 25px;
      transition: all 0.3s ease;
      border: 2px solid rgba(255, 255, 255, 0.5);
      font-weight: 600;
    }

    .project-link:hover {
      background-color: rgba(255, 255, 255, 0.3);
      transform: translateY(-2px);
      box-shadow: 0 4px 8px rgba(0,0,0,0.2);
    }

    .container {
      max-width: 900px;
      margin: 0 auto;
      padding: 20px;
      background-color: white;
      box-shadow: 0 0 20px rgba(0,0,0,0.1);
    }

    .section {
      margin-bottom: 40px;
    }

    .section-title {
      font-size: 1.8em;
      font-weight: 700;
      color: #fa709a;
      margin-bottom: 20px;
      border-bottom: 3px solid #fa709a;
      padding-bottom: 10px;
    }

    .abstract {
      font-size: 1.1em;
      line-height: 1.8;
      text-align: justify;
      color: #555;
      background-color: #fff9f0;
      padding: 25px;
      border-left: 4px solid #fa709a;
      margin-bottom: 30px;
    }

    .image-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
      gap: 20px;
      margin: 30px 0;
    }

    .image-item {
      text-align: center;
    }

    .image-item img {
      width: 100%;
      height: auto;
      border-radius: 8px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
      transition: transform 0.3s ease;
    }

    .image-item img:hover {
      transform: scale(1.05);
    }

    .image-caption {
      margin-top: 10px;
      font-size: 0.9em;
      color: #666;
      font-style: italic;
    }

    .back-link {
      display: inline-block;
      margin: 30px 0;
      padding: 10px 20px;
      background-color: #fa709a;
      color: white;
      text-decoration: none;
      border-radius: 5px;
      transition: all 0.3s ease;
    }

    .back-link:hover {
      background-color: #fee140;
      color: #333;
      transform: translateX(-5px);
    }

    .highlight-box {
      background-color: #fff9f0;
      border-left: 4px solid #fa709a;
      padding: 20px;
      margin: 20px 0;
      border-radius: 5px;
    }

    /* Dark theme support */
    [data-theme="dark"] body {
      background-color: #1a1a1a;
      color: #e0e0e0;
    }

    [data-theme="dark"] .container {
      background-color: #2d2d2d;
    }

    [data-theme="dark"] .abstract {
      background-color: #3a3a3a;
      color: #d0d0d0;
    }

    [data-theme="dark"] .section-title {
      color: #ff9fc4;
    }

    [data-theme="dark"] .highlight-box {
      background-color: #3a2f2a;
    }
  </style>
</head>

<body>
  <div class="project-header">
    <h1 class="project-title">6-DOF Robotic Grasping</h1>
    <p class="project-authors">
      <strong>Zihan Wang</strong>
    </p>
    <p class="project-affiliation">
      Tsinghua University
    </p>
    
    <div class="project-links">
      <a class="project-link" href="#" target="_blank">
        <i class="fa fa-file-pdf-o"></i> Paper (Coming Soon)
      </a>
      <a class="project-link" href="#" target="_blank">
        <i class="fa fa-github"></i> Code (Coming Soon)
      </a>
    </div>
  </div>

  <div class="container">
    <a href="../index.html#projects" class="back-link">
      <i class="fa fa-arrow-left"></i> Back to Home
    </a>

    <div class="section">
      <h2 class="section-title">Abstract</h2>
      <div class="abstract">
        We present a comprehensive approach to 6-degree-of-freedom (6-DOF) robotic grasping that enables robots to 
        grasp objects with arbitrary poses in three-dimensional space. Unlike traditional planar grasping methods 
        that are limited to top-down approaches, our system considers the full six dimensions of object pose: 
        three translational (x, y, z) and three rotational (roll, pitch, yaw) degrees of freedom. This capability 
        is crucial for manipulating objects in cluttered and unstructured environments. Our method combines deep 
        learning-based grasp detection with geometric reasoning and motion planning to achieve robust and reliable 
        grasping performance across diverse object categories.
      </div>
    </div>

    <div class="section">
      <h2 class="section-title">Key Features</h2>
      <ul style="font-size: 1.1em; line-height: 2; color: #555;">
        <li><strong>Full 6-DOF Pose Estimation:</strong> Complete spatial orientation and position for optimal grasping</li>
        <li><strong>Deep Learning Detection:</strong> Neural network-based grasp candidate generation</li>
        <li><strong>Point Cloud Processing:</strong> Direct operation on 3D sensor data for spatial understanding</li>
        <li><strong>Multi-Object Scenes:</strong> Capable of handling cluttered environments with multiple objects</li>
        <li><strong>Real-time Performance:</strong> Efficient computation for practical robotic applications</li>
        <li><strong>Generalization:</strong> Works across diverse object shapes and categories</li>
      </ul>
    </div>

    <div class="section">
      <h2 class="section-title">Grasping Examples</h2>
      <div class="image-grid">
        <div class="image-item">
          <img src="../images/6dof_grasp1.png" alt="6-DOF Grasp Example 1">
          <p class="image-caption">6-DOF grasp detection and execution in cluttered scene</p>
        </div>
        <div class="image-item">
          <img src="../images/6dof_grasp2.png" alt="6-DOF Grasp Example 2">
          <p class="image-caption">Multiple grasp candidates with quality evaluation</p>
        </div>
      </div>
    </div>

    <div class="section">
      <h2 class="section-title">Technical Approach</h2>
      
      <h3 style="color: #fa709a; margin-top: 20px;">1. 3D Perception</h3>
      <p style="font-size: 1.05em; line-height: 1.8; color: #555;">
        The system begins with 3D perception using depth cameras or RGB-D sensors to capture the scene geometry. 
        Point cloud data is processed to segment individual objects and extract relevant geometric features. This 
        spatial information forms the foundation for grasp planning in three-dimensional space.
      </p>

      <h3 style="color: #fa709a; margin-top: 20px;">2. Grasp Pose Generation</h3>
      <p style="font-size: 1.05em; line-height: 1.8; color: #555;">
        A deep neural network analyzes the point cloud to generate multiple grasp pose candidates. Each candidate 
        is represented as a full 6-DOF pose: position (x, y, z) and orientation (represented as a rotation matrix 
        or quaternion). The network learns to propose grasps that are geometrically feasible and likely to succeed 
        based on training data from successful grasps.
      </p>

      <div class="highlight-box">
        <strong>6-DOF Representation:</strong> Each grasp is defined by:
        <ul style="margin-top: 10px; margin-bottom: 0;">
          <li><strong>Translation:</strong> (x, y, z) position of the gripper center</li>
          <li><strong>Rotation:</strong> (roll, pitch, yaw) orientation of the gripper approach</li>
          <li><strong>Width:</strong> Gripper opening distance for the specific object</li>
          <li><strong>Quality Score:</strong> Predicted success probability for each grasp</li>
        </ul>
      </div>

      <h3 style="color: #fa709a; margin-top: 20px;">3. Grasp Quality Evaluation</h3>
      <p style="font-size: 1.05em; line-height: 1.8; color: #555;">
        Each generated grasp candidate is evaluated using multiple criteria including collision checking, force 
        closure analysis, and reachability assessment. The system considers gripper geometry, object shape, and 
        surrounding obstacles to rank grasps by their likelihood of success. Physics-based simulation can be used 
        to validate top candidates before execution.
      </p>

      <h3 style="color: #fa709a; margin-top: 20px;">4. Motion Planning and Execution</h3>
      <p style="font-size: 1.05em; line-height: 1.8; color: #555;">
        Once an optimal grasp is selected, the robot plans a collision-free trajectory from its current configuration 
        to the pre-grasp pose, then to the final grasp pose. The motion planner considers the full kinematic chain 
        of the robot arm, ensuring smooth and safe motion. After grasping, the robot lifts the object and transports 
        it to the desired location.
      </p>

      <h3 style="color: #fa709a; margin-top: 20px;">5. Adaptive Re-planning</h3>
      <p style="font-size: 1.05em; line-height: 1.8; color: #555;">
        If the initial grasp attempt fails or if the object slips during manipulation, the system can quickly 
        re-evaluate the scene and generate alternative grasp strategies. This adaptive capability enhances robustness 
        in real-world scenarios where sensor noise and object property uncertainties are common.
      </p>
    </div>

    <div class="section">
      <h2 class="section-title">Advantages of 6-DOF Grasping</h2>
      <div class="highlight-box">
        <ul style="font-size: 1.05em; line-height: 1.8; margin: 0;">
          <li><strong>Versatility:</strong> Can grasp objects from any angle, not just top-down approaches</li>
          <li><strong>Cluttered Scenes:</strong> Better handles densely packed objects by approaching from optimal angles</li>
          <li><strong>Object Variety:</strong> Adapts to diverse object shapes, sizes, and orientations</li>
          <li><strong>Human-like Manipulation:</strong> More natural and flexible grasping strategies</li>
          <li><strong>Task Efficiency:</strong> Reduces the need for object repositioning before grasping</li>
          <li><strong>Shelf and Bin Picking:</strong> Essential for grasping objects in constrained spaces</li>
        </ul>
      </div>
    </div>

    <div class="section">
      <h2 class="section-title">Applications</h2>
      <p style="font-size: 1.05em; line-height: 1.8; color: #555;">
        6-DOF robotic grasping technology has broad applications across multiple domains:
      </p>
      <ul style="font-size: 1.05em; line-height: 1.8; color: #555;">
        <li><strong>Warehouse Automation:</strong> Picking arbitrary items from shelves and bins for order fulfillment</li>
        <li><strong>Manufacturing:</strong> Assembly tasks requiring precise part manipulation from various orientations</li>
        <li><strong>Service Robots:</strong> Household robots fetching and manipulating everyday objects</li>
        <li><strong>Agriculture:</strong> Harvesting fruits and vegetables with delicate grasping</li>
        <li><strong>Healthcare:</strong> Assistive robots helping patients with object retrieval tasks</li>
        <li><strong>Waste Sorting:</strong> Robotic systems for recycling and waste management facilities</li>
      </ul>
    </div>

    <div class="section">
      <h2 class="section-title">Challenges and Solutions</h2>
      
      <h3 style="color: #fa709a; margin-top: 20px;">Occlusion Handling</h3>
      <p style="font-size: 1.05em; line-height: 1.8; color: #555;">
        Partial object visibility due to occlusions is addressed through learned shape completion networks that 
        infer the complete object geometry from partial observations. This enables grasp planning even when 
        significant portions of objects are hidden.
      </p>

      <h3 style="color: #fa709a; margin-top: 20px;">Real-time Computation</h3>
      <p style="font-size: 1.05em; line-height: 1.8; color: #555;">
        Computational efficiency is achieved through optimized neural network architectures and GPU acceleration. 
        The system processes point clouds and generates grasp candidates within seconds, suitable for interactive 
        robotic applications.
      </p>

      <h3 style="color: #fa709a; margin-top: 20px;">Generalization to Novel Objects</h3>
      <p style="font-size: 1.05em; line-height: 1.8; color: #555;">
        Training on diverse datasets and using data augmentation techniques enables the system to generalize to 
        previously unseen objects. The learned features capture fundamental grasp affordances that transfer across 
        different object categories.
      </p>
    </div>

    <div class="section">
      <h2 class="section-title">Future Work</h2>
      <p style="font-size: 1.05em; line-height: 1.8; color: #555;">
        Future developments include integration of tactile sensing for grasp refinement during execution, 
        incorporation of object semantics for task-oriented grasping (e.g., grasping a cup by the handle), 
        extension to dual-arm coordinated manipulation, and development of sim-to-real transfer techniques 
        to reduce the need for real-world training data. We also plan to explore reinforcement learning 
        approaches for continuous improvement of grasp strategies through interaction experience.
      </p>
    </div>

    <a href="../index.html#projects" class="back-link">
      <i class="fa fa-arrow-left"></i> Back to Home
    </a>
  </div>

  <script>
    // Theme toggle functionality
    const themeToggle = document.getElementById('theme-toggle');
    if (themeToggle) {
      themeToggle.addEventListener('click', function() {
        const currentTheme = document.documentElement.getAttribute('data-theme');
        const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
        document.documentElement.setAttribute('data-theme', newTheme);
        localStorage.setItem('theme', newTheme);
      });
    }

    // Load saved theme
    const savedTheme = localStorage.getItem('theme') || 'light';
    document.documentElement.setAttribute('data-theme', savedTheme);
  </script>
</body>

</html>
