<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>Mobile Car Projects - Zihan Wang</title>
  <meta name="description" content="Mobile Car Projects: Zhi Xing Car I, Zhi Xing Car II, and Auto-tracking Car by Zihan Wang">
  
  <link rel="shortcut icon" href="../images/android-chrome-512x512.png">
  <link rel="stylesheet" href="../main.css">
  <link rel="stylesheet" href="../css/style.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      color: #333;
      background-color: #ffffff;
      margin: 0;
      padding-top: 60px; /* ensure content sits below fixed header */
    }

    .container {
      max-width: 900px;
      margin: 0 auto;
      padding: 20px 20px;
    }

    h1 {
      text-align: center;
      font-size: 2em;
      margin-bottom: 15px;
      font-weight: 800;
      line-height: 1.2;
      font-family: "SimHei", "Heiti SC", "Microsoft YaHei", "Arial", sans-serif;
    }

    h2 {
      font-size: 1.5em;
      font-weight: 700;
      margin-top: 16px;
      margin-bottom: 12px;
      color: #1a1a1a;
      border-bottom: 2px solid #0066cc;
      padding-bottom: 5px;
    }

    h3 {
      margin-bottom: 10px;
      margin-top: 30px;
      text-align: left;
      font-size: 1.2em;
      font-weight: 600;
    }

    hr {
      border: none;
      border-top: 1px solid #ccc;
      margin: 30px 0;
    }

    .info-section {
      text-align: center;
      margin-bottom: 30px;
      line-height: 1.3;
    }

    .info-section nobr {
      display: inline-block;
      margin: 2px 10px;
    }

    .links {
      margin: 20px 0;
    }

    .links a {
      color: #0066cc;
      text-decoration: none;
      margin-right: 0; /* no trailing margin to avoid gap before closing bracket */
    }

    /* spacing between consecutive links only */
    .links a + a {
      margin-left: 15px;
    }

    .links a:hover {
      text-decoration: underline;
    }

    /* Main content justification: apply to paragraphs and lists only, keep headings alignment as-is */
    .container, .container p, .container .info-section, .container ul, .container ol, .container li {
      text-align: justify;
    }
    /* keep the info-section centered (override the container-level justify) */
    .container .info-section { text-align: center; }

    img {
      max-width: 50%;
      height: auto;
      display: block;
      margin: 15px auto;
    }

    .video-container {
      text-align: left;
      margin: 30px 0;
    }

    .video-container h3 {
      text-align: left;
    }

    .video-container iframe {
      max-width: 100%;
    }

    /* Project section styles */
    section {
      margin-bottom: 50px;
      padding: 20px 0;
    }

    .project-header {
      display: flex;
      align-items: flex-start;
      margin-bottom: 25px;
      gap: 30px;
    }

    .project-image {
      flex-shrink: 0;
      width: 300px;
    }

    .project-image img {
      width: 100%;
      height: auto;
      border-radius: 8px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
      margin: 0;
    }

    .project-info {
      flex: 1;
    }

    .project-info p {
      margin-bottom: 15px;
      text-align: justify;
    }

    /* Responsive design for mobile */
    @media (max-width: 768px) {
      .project-header {
        flex-direction: column;
        gap: 20px;
      }
      
      .project-image {
        width: 100%;
        max-width: 400px;
        margin: 0 auto;
      }
      
      section h2 {
        font-size: 1.5em;
      }
    }

    /* New project card styles */
    .project-card {
      display: flex;
      background: #ffffff;
      border-radius: 12px;
      box-shadow: 0 4px 16px rgba(0, 0, 0, 0.1);
      margin-bottom: 40px;
      padding: 24px;
      transition: transform 0.2s ease, box-shadow 0.2s ease;
      gap: 24px;
    }

    .project-card:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.15);
    }

    .dual-image-container {
      flex: 0 0 300px;
      display: flex;
      flex-direction: column;
      gap: 12px;
    }

    .dual-image-container img {
      width: 100%;
      height: 140px;
      object-fit: cover;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
      margin: 0;
    }

    .project-content {
      flex: 1;
      display: flex;
      flex-direction: column;
      gap: 16px;
    }

    .project-title {
      font-size: 1.5em;
      font-weight: 700;
      color: #2c3e50;
      margin: 0;
      line-height: 1.2;
    }

    .project-description {
      color: #555;
      line-height: 1.6;
      text-align: justify;
    }

    .tldr-label {
      font-weight: 700;
      color: #e74c3c;
    }

    .project-tags {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
    }

    .tag {
      padding: 4px 10px;
      border-radius: 16px;
      font-size: 0.8em;
      font-weight: 500;
      color: white;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .tag.hardware {
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    }

    .tag.software {
      background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
    }

    .tag.algorithm {
      background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
    }

    .project-links {
      display: flex;
      gap: 12px;
    }

    .project-link {
      display: inline-flex;
      align-items: center;
      gap: 6px;
      padding: 8px 16px;
      background: #3498db;
      color: white;
      text-decoration: none;
      border-radius: 6px;
      font-weight: 500;
      transition: background 0.2s ease;
    }

    .project-link:hover {
      background: #2980b9;
      color: white;
      text-decoration: none;
    }

    .project-link i {
      font-size: 0.9em;
    }

    /* Mobile responsive for project cards */
    @media (max-width: 768px) {
      .project-card {
        flex-direction: column;
        padding: 20px;
      }
      
      .dual-image-container {
        flex: none;
        max-width: 100%;
      }
      
      .dual-image-container img {
        height: 120px;
      }
      
      .project-title {
        font-size: 1.3em;
      }
      
      .project-links {
        flex-direction: column;
        gap: 8px;
      }
      
      .project-link {
        justify-content: center;
      }
    }

    .back-link {
      display: inline-block;
      margin: 20px 0;
      color: #0066cc;
      text-decoration: none;
    }

    .back-link:hover {
      text-decoration: underline;
    }

    /* Navbar styles moved to main.css to ensure consistent site-wide header */
  </style>
  <!-- MathJax for LaTeX rendering -->
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

<header class="site-header" style="border: 2px solid lightgrey;">
  <div class="wrapper">
    <nav class="site-nav">
      <div class="trigger">
        <a class="page-link" href="../index.html"><b>Home</b></a>
        <a class="page-link" href="../index.html#research"><b>Research</b></a>
        <a class="page-link" href="../index.html#projects"><b>Projects</b></a>
        <a class="page-link" href="../blog.html"><b>Blog</b></a>
      </div>
    </nav>
  </div>
</header>



<div class="container">
<div class="info-section">
  <center><h1>Mobile Car Projects</h1></center>
</div>
<!-- Project 1: Zhi Xing Car I -->
<section id="zhixing-car-1">
  <div style="display: flex; align-items: flex-start; margin-bottom: 8px;">
    <img src="../images/zhixing.png" alt="Zhi Xing Car I" style="width: 300px; height: auto; margin-right: 30px; margin-left: 0; margin-top: 0; margin-bottom: 0;">
    <div>
      <h2 style="margin-bottom: 15px; color: #333;">Zhi Xing Car I - SLAM Navigation Robot</h2>
      <p style="margin-bottom: 10px; font-size: 1.1em; line-height: 1.5;">
        A comprehensive autonomous navigation robot system integrating SLAM mapping, path planning, voice control, 
        and face recognition. Built on ROS framework with LiDAR-based localization and multi-sensor fusion, 
        achieving robust autonomous navigation and intelligent interaction in indoor environments.
      <div class="links" style="margin-top: 15px;">
        [<a href="../work/zhixingmini.pdf" target="_blank">Document</a>]
      </div>
    </div>
  </div>

    <div class="video-container" style="margin: 20px 0;">
    <h2>Video</h2>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/74ppMMNvv9I" frameborder="0" allow="accelerometer; autoplay; encrypted-media" allowfullscreen></iframe>
  </div>

  <h2>1. Vehicle Kinematics Analysis</h2>
  <p style="margin-bottom: 12px; font-size: 1.05em;">This section summarizes the differential-drive vehicle kinematics and control models, including forward/inverse kinematics, relationships between wheel speeds and robot linear/angular velocities, and common trajectory tracking algorithms.</p>

  <!-- Inserted: English summaries for Document Chapters 6,7,8 -->
  <h3 style="margin-top:16px;">1.1 Odometry & Coordinate Transforms</h3>
  <p style="margin-bottom:10px;">In ROS-based robots the most common frames are <code>map</code>, <code>odom</code> and <code>base_link</code>. The <code>map</code> frame is a fixed, long-term global reference (Z axis up). Because localization may apply discrete corrections when new sensor data arrives, poses expressed in <code>map</code> can occasionally "jump"; this makes <code>map</code> suitable as a global reference but not ideal as a fast local control frame.</p>
  <p style="margin-bottom:10px;">The <code>odom</code> frame is a locally-continuous reference produced by dead-reckoning sources (wheel encoders, visual odometry, IMU). <code>odom</code> guarantees smooth, continuous pose updates (no sudden jumps) and is useful as a short-term reference for sensors and controllers, but it accumulates drift and therefore cannot serve as a precise long-term global frame. Typically <code>map</code> and <code>odom</code> start aligned; localization modules compute the transform <code>map → odom</code> (via <code>tf</code>) to correct odometry drift.</p>
  <p style="margin-bottom:10px;">Note the distinction between the <code>odom</code> coordinate frame and an <code>/odom</code> topic: the frame is the coordinate reference, whereas the <code>/odom</code> topic commonly publishes the transform from <code>odom</code> to <code>base_link</code> (i.e., the robot pose in the <code>odom</code> frame).</p>
  <p style="margin-bottom:10px;">The <code>base_link</code> frame is the robot body frame (usually centered on the platform); the core localization task is estimating <code>base_link</code>'s pose in the <code>map</code> frame. Each sensor has its own fixed sensor frame (for example <code>laser</code> or <code>base_laser</code> for a LiDAR), and the transform between that sensor frame and <code>base_link</code> is constant and should be published via <code>tf</code>.</p>
 
  <!-- frames diagram -->
  <div style="text-align:center; margin: 18px 0;">
    <svg xmlns="http://www.w3.org/2000/svg" width="900" height="420" viewBox="0 0 900 420" style="max-width:100%; height:auto; border:1px solid #e0e0e0; padding:6px; background:#fff;">
      <style>
        .frame-box { fill: #f7fbff; stroke: #2b6cb0; stroke-width: 2px; rx: 8px; }
        .frame-label { font-family: Arial, sans-serif; font-size: 16px; fill: #0b3d91; font-weight: 700; }
        .frame-desc { font-family: Arial, sans-serif; font-size: 12px; fill: #333333; }
        .arrow { stroke: #2b6cb0; stroke-width: 2px; fill: none; marker-end: url(#arrowhead); }
        .note { font-family: Arial, sans-serif; font-size: 12px; fill: #444; }
      </style>

      <defs>
        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="10" refY="3.5" orient="auto">
          <polygon points="0 0, 10 3.5, 0 7" fill="#2b6cb0" />
        </marker>
      </defs>

      <!-- map frame -->
      <rect x="40" y="30" width="220" height="120" class="frame-box" />
      <text x="150" y="60" text-anchor="middle" class="frame-label">map</text>
      <text x="150" y="82" text-anchor="middle" class="frame-desc">Global fixed frame</text>
      <text x="150" y="100" text-anchor="middle" class="frame-desc">(long-term reference)</text>

      <!-- odom frame -->
      <rect x="340" y="30" width="220" height="120" class="frame-box" />
      <text x="450" y="60" text-anchor="middle" class="frame-label">odom</text>
      <text x="450" y="82" text-anchor="middle" class="frame-desc">Local continuous frame</text>
      <text x="450" y="100" text-anchor="middle" class="frame-desc">(short-term, may drift)</text>

      <!-- base_link frame -->
      <rect x="640" y="30" width="220" height="120" class="frame-box" />
      <text x="750" y="60" text-anchor="middle" class="frame-label">base_link</text>
      <text x="750" y="82" text-anchor="middle" class="frame-desc">Robot body frame</text>
      <text x="750" y="100" text-anchor="middle" class="frame-desc">(pose expressed here)</text>

      <!-- arrows for transforms -->
      <path d="M260 90 L340 90" class="arrow" />
      <text x="300" y="86" text-anchor="middle" class="note">map → odom</text>
      <text x="300" y="108" text-anchor="middle" class="note">(tf correction)</text>

      <path d="M560 90 L640 90" class="arrow" />
      <text x="600" y="86" text-anchor="middle" class="note">odom → base_link</text>
      <text x="600" y="108" text-anchor="middle" class="note">(odom topic)</text>

      <!-- sensor (laser) -->
      <rect x="640" y="200" width="220" height="80" class="frame-box" />
      <text x="750" y="230" text-anchor="middle" class="frame-label">laser</text>
      <text x="750" y="250" text-anchor="middle" class="frame-desc">LiDAR sensor frame</text>
      <text x="750" y="268" text-anchor="middle" class="frame-desc">(fixed transform to base_link)</text>

      <!-- connect vertical arrow so it meets the top edge of base_link box (y=30) + height 120 => top at y=30, bottom at y=150; we want arrow to meet top of laser box at y=200 and connect to base_link at y=150 -->
      <path d="M750 150 L750 200" class="arrow" />
      <text x="770" y="176" class="note"><tspan x="770" dy="0">fixed</tspan><tspan x="770" dy="16">TF</tspan></text>

      <!-- additional note -->
      <text x="40" y="360" class="note">Note: map may "jump" when localization updates; odom provides smooth short-term pose but accumulates drift. Use sensor fusion (IMU, AMCL) to compute map→odom corrections.</text>
    </svg>
    <img src="../images/mobile_car/tf.png" alt="Frames diagram (map, odom, base_link, laser)" style="max-width:100%; height:auto; border:1px solid #e0e0e0; padding:6px; background:#fff;" />
  </div>

  <h3 style="margin-top:16px;">1.2 Kinematic Transformation from Robot Velocity to Motor Speeds</h3>
  The kinematic model of the differential wheel chassis is established as follows:
  <div style="text-align:center; margin: 18px 0;"></div>
    <img src="../images/mobile_car/car_dynamic.png" alt="Car dynamics diagram" style="max-width:100%; height:auto; border:1px solid #e0e0e0; padding:6px; background:#fff;" />

    <!-- Extracted derivation and formulas (English, LaTeX preserved) -->
    <p style="margin-bottom:10px;">Below are the key points extracted from the image. Let the linear speeds of the two wheels be <code>V_a</code> (wheel A) and <code>V_b</code> (wheel B), and let the track width be <code>2L</code>. When only one wheel is turning, the contact point of the stationary wheel is the instantaneous center of rotation (ICR), which yields special-case kinematic relations.</p>
      <img src="../images/mobile_car/car_dynamic2.png" alt="Car dynamics diagram" style="max-width:100%; height:auto; border:1px solid #e0e0e0; padding:6px; background:#fff;" />
    <p style="margin-bottom:8px;">1) Only wheel A rotates (wheel B stationary):</p>
    <p style="margin-bottom:10px; text-align:center; font-style:italic;">$$V_{ox}=\frac{V_a}{2},\quad V_{oy}=0,\quad \dot{\theta}=-\frac{V_a}{2L}$$</p>

  <p style="margin-bottom:8px;">2) Only wheel B rotates (wheel A stationary):</p>
  <p style="margin-bottom:10px; text-align:center; font-style:italic;">$$
  \begin{cases}
  V_{ox}=\frac{V_b}{2}\\
  V_{oy}=0\\
  \dot{\theta}=\frac{V_b}{2L}
  \end{cases}
  $$</p>

  <p style="margin-bottom:8px;">Combining the two special cases gives the general form (superposition of wheel speeds):</p>
  <p style="margin-bottom:10px; text-align:center; font-style:italic;">$$
  \begin{cases}
  V_{ox}=\tfrac{1}{2}(V_a+V_b)\\
  V_{oy}=0\\
  \dot{\theta}=\tfrac{1}{2L}(V_b-V_a)
  \end{cases}
  $$</p>
  <p>Based on the forward kinematic formulation, the corresponding inverse kinematic equations can be directly obtained, establishing the mapping from the robot’s body-frame velocity to each wheel’s linear velocity:</p>
  <p style="text-align:center; font-style:italic;">$$
  \begin{cases}
  V_a = V_{ox} + L\,\dot{\theta}\\
  V_b = V_{ox} - L\,\dot{\theta}
  \end{cases}
  $$</p>

    <h3 style="margin-top:16px;">1.3 Kinematic Transformation from Motor Speeds to Robot Displacement</h3>
  <!-- Inserted additional diagram and derived relations from attachment -->
  <div style="text-align:center; margin: 18px 0;">
    <img src="../images/mobile_car/car_dynamic3.png" alt="Car dynamics diagram 3" style="max-width:100%; height:auto; border:1px solid #e0e0e0; padding:6px; background:#fff;" />
  </div>

  <p style="margin-bottom:10px;">The following summarizes the coordinate transform and velocity relationships illustrated in the diagram above. Let the robot body frame be (x',y') rotated by heading \(\theta\) relative to the global frame (x,y). The coordinate transform from body to global is:</p>
  <p style="margin-bottom:10px; text-align:center; font-style:italic;">$$x = x'\cos\theta - y'\sin\theta\\
  y = x'\sin\theta + y'\cos\theta$$</p>

  <p style="margin-bottom:10px;">For the two-wheel differential drive with wheel linear speeds \(V_a\) and \(V_b\) (left/right or labeled in the figure), the derived body-frame velocity components are:</p>
  <p style="margin-bottom:10px; text-align:center; font-style:italic;">$$
  \begin{cases}
  V_{ox}=\tfrac{1}{2}(V_a+V_b)\cos\phi\\
  V_{oy}= -\tfrac{1}{2}(V_a+V_b)\sin\phi\\
  \dot{\theta}=\tfrac{1}{2L}(V_b-V_a)
  \end{cases}
  $$</p>

  This set of equations represents the transformation from the angular velocities of the two motors to the robot’s motion velocity in the global (world) coordinate frame.
During odometry computation, the robot’s linear and angular velocities are integrated over each time interval, and the accumulated result yields the current displacement, which corresponds to the data published on the <code>/odom</code> topic.
With this, the derivation of the differential-drive kinematic model is complete.
 

  <h2>2. SLAM Navigation Workflow</h2>

  <h3 style="margin-top:16px;">2.1 System Architecture & Hardware Setup</h3>
  <ul style="text-align:left; list-style-type:disc; margin-top:8px; margin-left:20px;">
    <li><strong>Computing Platform:</strong> On-board computer with ROS Melodic on Ubuntu 18.04</li>
    <li><strong>Sensors:</strong> RPLiDAR A1 (360° laser scanner), ReSpeaker voice array, RGB-D camera</li>
    <li><strong>Motor Control:</strong> Differential drive system with encoder feedback</li>
    <li><strong>Communication:</strong> USB serial communication with custom protocol for motor control</li>
  </ul>


  <h3 style="margin-top: 20px; margin-bottom: 10px; color: #555;">2.2 GMapping Algorithm Based on RBPF</h3>
  <img src="../images/mobile_car/gmapping.png" alt="Car dynamics diagram 3" style="max-width:100%; height:auto; border:1px solid #e0e0e0; padding:6px; background:#fff;" />

    <p>GMapping is a widely used SLAM (Simultaneous Localization and Mapping) algorithm based on the <strong>Rao-Blackwellized Particle Filter (RBPF)</strong> framework.</p>
    <p>It factorizes the full SLAM posterior into two parts: the robot trajectory estimation and the map estimation.</p>

    <p style="text-align:center; font-style:italic;">$$
    p(x_{1:t}, m \mid z_{1:t}, u_{1:t-1}) = p(m \mid x_{1:t}, z_{1:t}) \, p(x_{1:t} \mid z_{1:t}, u_{1:t-1})
    $$</p>

    <p>where:</p>
    <ul>
      <li><span style="white-space:nowrap;">\(x_{1:t}\)</span>: the robot trajectory up to time <span style="white-space:nowrap;">\(t\)</span>;</li>
      <li><span style="white-space:nowrap;">\(m\)</span>: the map of the environment;</li>
      <li><span style="white-space:nowrap;">\(z_{1:t}\)</span>: the sequence of sensor measurements (e.g., LiDAR scans);</li>
      <li><span style="white-space:nowrap;">\(u_{1:t-1}\)</span>: the sequence of control inputs (odometry).</li>
    </ul>

Since the map \(m\) can be determined given the trajectory \(x_{1:t}\), the SLAM problem reduces to estimating the trajectory posterior using a particle filter.

<!-- GMapping: improved proposal and selective resampling explanation -->
<h4 style="margin-top:12px;">1. Improved Proposal Distribution</h4>
<p>In traditional FastSLAM 1.0, the proposal distribution for sampling new particle poses only depends on the motion model:</p>
<p style="text-align:center; font-style:italic;">$$p(x_t\mid x_{t-1}, u_t)$$</p>
<p>This approach suffers from high uncertainty due to odometry drift and noise, often causing <strong>particle degeneracy</strong> — only a few particles have significant weights.</p>
<p>GMapping improves this by incorporating the current observation \(z_t\) into the proposal distribution:</p>
<p style="text-align:center; font-style:italic;">$$p(x_t\mid x_{t-1}, u_t, z_t)$$</p>
<p><strong>Key idea:</strong> combining motion and observation information corrects the motion-model prediction using the latest laser scan, reducing uncertainty of sampled particles.</p>
<p><strong>Implementation steps:</strong></p>
<ol>
  <li>For each particle \(i\), predict a set of pose candidates using the motion model.</li>
  <li>Evaluate each candidate by computing the likelihood \(p(z_t\mid x_t^{(i)}, m_{t-1}^{(i)})\) through scan matching.</li>
  <li>Approximate this likelihood as a Gaussian distribution around the best-matched pose.</li>
  <li>Sample the new particle pose from this corrected Gaussian proposal.</li>
</ol>
<p>The particle weight update becomes:</p>
<p style="text-align:center; font-style:italic;">$$w_t^{(i)} \propto w_{t-1}^{(i)} \dfrac{p(z_t\mid x_t^{(i)}, m_{t-1}^{(i)})\, p(x_t^{(i)}\mid x_{t-1}^{(i)}, u_t)}{q(x_t^{(i)}\mid x_{t-1}^{(i)}, u_t, z_t)}$$</p>
<p><strong>Effect:</strong></p>
<ul>
  <li>Reduces particle dispersion and improves localization accuracy.</li>
  <li>Requires fewer particles (typically tens instead of hundreds).</li>
  <li>Increases robustness against odometry drift.</li>
</ul>

<h4 style="margin-top:12px;">2. Selective Resampling</h4>
<p>In a standard particle filter, resampling is performed at every iteration to prevent weight degeneracy. Frequent resampling can cause <strong>sample impoverishment</strong>, reducing particle diversity and potentially leading to local minima. GMapping uses selective resampling:</p>
<p style="text-align:center; font-style:italic;">Only perform resampling when the effective sample size \(N_{\text{eff}}\) falls below a threshold.</p>
<p>The effective sample size is defined as:</p>
<p style="text-align:center; font-style:italic;">$$N_{\text{eff}} = \dfrac{1}{\sum_{i=1}^{N} (w_t^{(i)})^2}$$</p>
<p>When \(N_{\text{eff}} < N_{\text{threshold}}\) (typically \(N/2\) or \(N/3\)), resampling is triggered.</p>
<p><strong>Benefits:</strong></p>
<ul>
  <li>Maintains particle diversity.</li>
  <li>Avoids unnecessary resampling when the weight distribution is stable.</li>
  <li>Improves convergence speed and map consistency.</li>
</ul>

<!-- End GMapping section -->




 <h3 style="margin-top: 20px; margin-bottom: 10px; color: #555;">2.3 Navigation</h3>


 <h4 style="margin-top:12px;">1.Local Navigfa </h4>



</section>

  <!-- References for Project 1 -->
  <section id="zhixing-car-1-references">
    <h3 style="margin-top:12px;">References</h3>
    <ol>
  <li>G. Grisetti, C. Stachniss, and W. Burgard, "Improving grid-based SLAM with Rao-Blackwellized particle filters by adaptive proposals and selective resampling," in Proc. IEEE Int. Conf. Robotics and Automation (ICRA), 2005, pp. 2432–2437.</li>
    </ol>
  </section>

<br>

<!-- Project 2: Zhi Xing Car II -->
<section id="zhixing-car-2">
  <div style="display: flex; align-items: flex-start; margin-bottom: 20px;">
    <img src="../images/vla_car.jpg" alt="Zhi Xing Car II" style="width: 300px; height: auto; margin-right: 30px; margin-left: 0; margin-top: 0; margin-bottom: 0;">
    <div>
      <h2 style="margin-bottom: 15px; color: #333;">Zhi Xing Car II</h2>
      <p style="margin-bottom: 10px; font-size: 1.1em; line-height: 1.5;">
        We designed and deployed a SLAM system on NVIDIA Jetson®, integrating 2D LiDAR 
        and Intel® RealSense™ depth camera for robust multi-sensor fusion. Implemented Google Cartographer 
        for high-accuracy 2D localization and mapping, and extended to 3D ESDF-based mapping for precise 
        environmental reconstruction and optimized path planning in real-world navigation tasks.
      </p>
      <div class="links" style="margin-top: 15px;">
        [<a href="https://github.com/EmbodiedLLM/roadRunner" target="_blank">Code</a>]
      </div>
    </div>
  </div>

  <h3 style="margin-bottom:10px;">Key Technical Innovations</h3>
  <ul>
    <li><strong>NVIDIA Jetson Nano:</strong> High-performance embedded computing for real-time processing</li>
    <li><strong>Intel RealSense D435:</strong> Advanced depth camera for 3D perception</li>
    <li><strong>MID-360 LiDAR:</strong> 360-degree laser scanning for precise mapping</li>
    <li><strong>ROS2:</strong> Next-generation robotics middleware for distributed systems</li>
    <li><strong>Navigation2:</strong> Advanced navigation stack for autonomous mobile robots</li>
    <li><strong>Google Cartographer:</strong> Advanced SLAM algorithm for high-accuracy localization</li>
    <li><strong>3D ESDF Mapping:</strong> Extended Signed Distance Field for precise environmental modeling</li>
  </ul>

  <img src="../images/NVBlox.png" alt="Zhi Xing Car II SLAM" style="max-width: 70%; margin: 20px auto; display: block;">
</section>

<br>
<!-- Project 2: Auto-tracking Car -->
<section id="auto-tracking-car">
  <div style="display: flex; align-items: flex-start; margin-bottom: 20px;">
    <img src="../images/yuyuan.jpg" alt="Auto-tracking Car" style="width: 300px; height: auto; margin-right: 30px; margin-left: 0; margin-top: 0; margin-bottom: 0;">
    <div>
      <h2 style="margin-bottom: 15px; color: #333;">Auto-tracking Car</h2>
      <p style="margin-bottom: 10px; font-size: 1.1em; line-height: 1.5;">
        We used OpenCV to rectify the camera-captured map and extract obstacle 
        coordinates for path planning. Based on the map, we designed an efficient motion planning algorithm 
        using A* search algorithm, enabling the car to avoid randomly positioned obstacles. Then we 
        implemented a PID controller for precise speed and steering control, ensuring smooth navigation.
      </p>
      <div class="links" style="margin-top: 15px;">
        [<a href="https://github.com/ZihanWang0422/Treasure_Hunting_Car" target="_blank">Code</a>]
      </div>
    </div>
  </div>

    <div class="video-container" style="margin: 20px 0;">
    <h3 style="margin-bottom:10px;">Video</h3>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/R3W2jP_Io_M" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
  </div>

  <h3 style="margin-bottom:10px;">Technical Highlights</h3>
  <ul>
    <li><strong>Arduino & ESP32:</strong> Embedded microcontroller platform for real-time control</li>
    <li><strong>PlatformIO:</strong> Development environment for embedded systems</li>
    <li><strong>OpenCV Integration:</strong> Real-time image processing for map rectification and analysis</li>
    <li><strong>A* Path Planning:</strong> Optimal path finding algorithm for efficient navigation</li>
    <li><strong>PID Control System:</strong> Precise speed and steering control for smooth motion</li>
    <li><strong>Dynamic Obstacle Avoidance:</strong> Real-time adaptation to randomly positioned obstacles</li>
  </ul>

</section>


<hr>

</body>
</html>